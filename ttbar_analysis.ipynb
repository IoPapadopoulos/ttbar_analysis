{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPBhiipJLKboRbPu/+8Y2uU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IoPapadopoulos/ttbar_analysis/blob/main/ttbar_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**How to Rediscover the Top-Antitop $t\\bar{t}$ Quark Pair Production Yourself!**\n",
        "\n",
        "This notebook uses ATLAS Open Data to guide you through the steps needed to rediscover the production of top-antitop quark pairs $t\\bar{t}$ in high-energy proton-proton collisions at the Large Hadron Collider (LHC).\n",
        "\n",
        "ATLAS Open Data provides open access to proton-proton collision data collected by the ATLAS experiment at the LHC. These datasets are made available for educational purposes, making them ideal for high-school, undergraduate, and postgraduate students interested in learning about particle physics.\n",
        "\n",
        "# **What Are Notebooks?**\n",
        "\n",
        "Notebooks are interactive web applications that allow you to create and share documents that contain:\n",
        "\n",
        "1. **Live code:** Write and execute code in real-time, making adjustments as you go.\n",
        "\n",
        "2. **Visualizations:** Create plots, histograms, and other graphical representations of your data to better understand the underlying physics.\n",
        "\n",
        "3. **Narrative text:** Include explanations, descriptions, and commentary to guide yourself or others through the analysis.\n",
        "\n",
        "\n",
        "# **The Goal: Rediscovering $t\\bar{t}$ Production**\n",
        "\n",
        "By following this notebook, you will perform a $t\\bar{t}$ analysis, aiming to identify and study the production of top-antitop quark pairs. The process involves applying a series of selection cuts and analysis techniques to increase the ratio of signal (events where $t\\bar{t}$ pairs are produced) to background (other processes that can mimic the signal).\n",
        "\n",
        "A key part of this analysis is focusing on the semileptonic decay of the $t\\bar{t}$ pair. In this decay mode, one top quark decays into a W boson and a b-quark, with the W boson further decaying into a charged lepton and a neutrino, while the other top quark decays hadronically (into jets).This can be represented as:\n",
        "* $ t\\bar{t} \\rightarrow lvb\\bar{b}q\\bar{q}$\n",
        "\n",
        "where:\n",
        "\n",
        "* $l$ is a lepton,\n",
        "* $v$ is a neutrino,\n",
        "* $b$ is a b-tagged jet,\n",
        "* $q$ is a jet.\n",
        "\n",
        "\n",
        "**Contents:**\n",
        "* Running a Jupyter notebook\n",
        "* To setup\n",
        "* Explanation of Key Parameters in the $t\\bar{t}$ Analysis Code\n",
        "* Samples\n",
        "* Weight in Particle Physics Analysis\n",
        "* Introduction to Event Selection Cuts in $t\\bar{t}$ Analysis\n",
        "* Introduction to Mass Reconstruction in $t\\bar{t}$ Analysis\n",
        "* Data Processing and Event Selection Function\n",
        "* Data Retrieval in $t\\bar{t}$ Analysis\n",
        "* Data Processing\n",
        "* Data Aggregation with Loops\n",
        "* Combining Mass and Weight Lists\n",
        "* Plotting\n",
        "\n",
        "\n",
        "# **Running a Jupyter notebook**\n",
        "To run the whole Jupyter notebook, in the top menu click Cell -> Run All.\n",
        "\n",
        "To propagate a change you've made to a piece of code, click Cell -> Run All Below.\n",
        "\n",
        "You can also run a single code cell, by clicking Cell -> Run Cells, or using the keyboard shortcut Shift+Enter.\n"
      ],
      "metadata": {
        "id": "v9CDWygzhxRE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install uproot"
      ],
      "metadata": {
        "id": "63X2lXpgywvZ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **To setup**\n",
        "Cell -> Run All Below\n",
        "\n",
        "to be done every time you re-open this notebook\n",
        "\n",
        "We're going to be using a number of tools to help us:\n",
        "\n",
        "* uproot: lets us read .root files typically used in particle physics into data formats used in python\n",
        "* awkward: lets us use efficiently the nested data in columnar format\n",
        "* pandas: lets us store data as dataframes, a format widely used in python\n",
        "* numpy: provides numerical calculations such as histogramming\n",
        "* matplotlib: common tool for making plots, figures, images, visualisations\n",
        "lmfit: tool for statistical fitting"
      ],
      "metadata": {
        "id": "f477h5dJ-SFM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import uproot  # For reading ROOT files efficiently\n",
        "\n",
        "import awkward as ak  # To represent nested data in columnar format\n",
        "import numpy as np  # For numerical calculations such as histogramming\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt  # For creating plots and visualizations\n",
        "from matplotlib.ticker import AutoMinorLocator  # For adding minor ticks to plot axes\n",
        "\n",
        "import requests  # For making HTTP requests\n",
        "\n",
        "import time  # For timing operations and adding delays if needed\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", message=\"invalid value encountered in sqrt\")\n",
        "warnings.filterwarnings(\"ignore\", message=\"overflow encountered in power\")\n",
        "warnings.filterwarnings(\"ignore\", message=\"overflow encountered in multiply\", category=RuntimeWarning)\n",
        "warnings.filterwarnings(\"ignore\", message=\"invalid value encountered in subtract\")"
      ],
      "metadata": {
        "id": "y7inFXIrVWMI"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Explanation of Key Parameters in the $t\\bar{t}$ Analysis Code**\n",
        "\n",
        "In particle physics analyses, various parameters and constants are essential for accurately processing data and interpreting results. Below is an explanation of the key parameters used in the\n",
        " $t\\bar{t}$ analysis code provided:\n",
        "\n",
        "1.   **Integrated Luminosity (lumi)**\n",
        "\n",
        "  *   Definition: Integrated luminosity is a measure of the total amount of data collected by a particle detector over a certain period. It represents the total number of potential collisions that could have occurred in a particle accelerator and is typically measured in inverse femtobarns fb$^{-1}$.\n",
        "\n",
        "2. **Fraction of Events to Process**\n",
        "\n",
        "  *   Definition: This parameter controls what fraction of the available events in the dataset will be processed by the analysis.\n",
        "\n",
        "3. **Conversion Factor from MeV to GeV**\n",
        "\n",
        "  * Definition: Particle physics often uses energy units like MeV (Mega electron Volts) and GeV (Giga electron Volts). The conversion factor helps switch between these units.\n",
        "\n",
        "4. **Minimum Transverse Momentum $p_ùëá$ Limit Definition**\n",
        "  * Transverse momentum $p_ùëá$ is a crucial quantity in particle physics, representing the momentum of particles perpendicular to the beam axis. It's often used as a selection criterion for events.\n",
        "\n",
        "5. **MV2c10 b-Tagging Algorithm Discriminant Cut Value Definition**\n",
        "  * The MV2c10 is a b-tagging algorithm used to identify jets originating from b-quarks. The discriminant cut value determines the threshold for tagging a jet as a b-jet.\n",
        "\n",
        "  For further information visit [atlas glossary](https://atlas.cern/glossary)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NeWGPZrCeWTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Integrated luminosity in inverse femtobarns\n",
        "lumi = 10000.\n",
        "\n",
        "# Fraction of events to process\n",
        "fraction = 0.1\n",
        "\n",
        "# Conversion factor from MeV to GeV\n",
        "MeV2GeV = 0.001\n",
        "\n",
        "# Minimum transverse momentum (pT) limit in GeV\n",
        "pt_lim = 30\n",
        "\n",
        "# MV2c10 b-tagging algorithm discriminant cut value\n",
        "MV2c10_lim = 0.828"
      ],
      "metadata": {
        "id": "rMdNPb42cNA5"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Samples**\n",
        "Samples to process"
      ],
      "metadata": {
        "id": "u6CY1t7X1YzH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionary of samples to be processed\n",
        "samples = {\n",
        "    # Real data samples\n",
        "    'data_A': {'list' : ['data_A'],},\n",
        "    'data_B': {'list' : ['data_B'],},\n",
        "    'data_C': {'list' : ['data_C'],},\n",
        "    'data_D': {'list' : ['data_D'],},\n",
        "\n",
        "    # Main Monte Carlo (MC) sample: top quark pair production\n",
        "    'ttbar' : {  'list' : ['mc_410000.ttbar_lep'],},\n",
        "\n",
        "    # Single top quark production samples\n",
        "    'Single top' : {  'list' : [\n",
        "        'mc_410011.single_top_tchan',      # t-channel single top\n",
        "        'mc_410012.single_antitop_tchan',  # t-channel single anti-top\n",
        "        'mc_410013.single_top_wtchan',     # Wt-channel single top\n",
        "        'mc_410014.single_antitop_wtchan', # Wt-channel single anti-top\n",
        "        'mc_410025.single_top_schan',      # s-channel single top\n",
        "        'mc_410026.single_antitop_schan'   # s-channel single anti-top\n",
        "        ],},\n",
        "\n",
        "    # Diboson production samples\n",
        "    'Diboson' : {  'list' : [\n",
        "        'mc_363356.ZqqZll',    # Z(qq)Z(ll)\n",
        "        'mc_363358.WqqZll',    # W(qq)Z(ll)\n",
        "        'mc_363359.WpqqWmlv',  # W+(qq)W-(lv)\n",
        "        'mc_363360.WplvWmqq',  # W+(lv)W-(qq)\n",
        "        'mc_363359.WpqqWmlv',  # W+(qq)W-(lv)\n",
        "        'mc_363360.WplvWmqq',  # W+(lv)W-(qq)\n",
        "        'mc_363489.WlvZqq'     # W(lv)Z(qq)\n",
        "         ],},\n",
        "        }"
      ],
      "metadata": {
        "id": "g7G4qeaGy9p1"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Weight in Particle Physics Analysis**\n",
        "\n",
        "In particle physics analysis, such as in the study of the production of top-antitop quark pairs $t\\bar{t}$, the concept of \"weight\" plays a crucial role. Weights are factors applied to events or data points in a dataset to ensure that the results of an analysis accurately reflect the underlying physics being studied. These weights account for various factors, including the efficiencies of detectors, the probability of certain processes occurring, and the corrections needed to match the simulated data with real-world observations.\n",
        "\n",
        "The following function computes the event weights by combining several correction factors, including trigger efficiencies, pileup corrections, b-tagging efficiencies, and cross-sections. The result is a set of weights that accurately reflect the likelihood and significance of each event, ensuring that the final analysis properly accounts for all relevant physical and experimental considerations."
      ],
      "metadata": {
        "id": "2Wf_zDJbcTxw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_weight(data):\n",
        "    # Calculate the common scale factor\n",
        "    scale = (data[\"scaleFactor_LepTRIGGER\"] * data[\"scaleFactor_PILEUP\"] * data[\"scaleFactor_BTAG\"] *\n",
        "             (data[\"mcWeight\"] / data[\"SumWeights\"]) * (data[\"XSection\"] * lumi))\n",
        "\n",
        "    # Calculate the scaleFactorE and scaleFactorM\n",
        "    scaleFactorE = data[\"scaleFactor_ELE\"] * scale\n",
        "    scaleFactorM = data[\"scaleFactor_MUON\"] * scale\n",
        "\n",
        "    # Create boolean masks based on conditions\n",
        "    e_condition = data[\"trigE\"] & (data[\"mcWeight\"] != 0.0)\n",
        "\n",
        "    # Use boolean masks to select relevant entries from scaleFactorE and scaleFactorM\n",
        "    weight_list = ak.where(e_condition, scaleFactorE, scaleFactorM)\n",
        "\n",
        "    return weight_list"
      ],
      "metadata": {
        "id": "hcbBI4BAcTF7"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Introduction to Event Selection Cuts in $t\\bar{t}$ Analysis**\n",
        "\n",
        "In particle physics, particularly in the analysis of top-antitop $t\\bar{t}$ quark pair production, the process of event selection is crucial for isolating the signal from the background. The goal is to apply a series of cuts, or selection criteria, that filter out events unlikely to be associated with the $t\\bar{t}$ process, leaving a dataset enriched with signal events. These cuts are based on specific physical properties of the events, such as the presence and characteristics of leptons, missing transverse energy $E^{miss}_{T}$, and jets.\n",
        "\n",
        "Each cut is designed to enhance the signal-to-background ratio, which is essential for making a clear observation of the $t\\bar{t}$ signal. Below is an overview of the key cuts applied in this analysis:\n",
        "\n",
        "1. **Trigger Selection:**\n",
        "  * The first step in the analysis is to ensure that the events under consideration have fired the appropriate triggers.\n",
        "  * Purpose: This cut checks whether either the electron trigger ('trigE') or the muon trigger ('trigM') has been activated. If either trigger fired, the event is considered for further analysis."
      ],
      "metadata": {
        "id": "OsUDeftK2kkx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cut_trig(trigE, trigM):\n",
        "    # Return True if either electron or muon trigger fired\n",
        "    return trigE | trigM"
      ],
      "metadata": {
        "id": "xQVmJwTQrpNT"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Single Lepton Requirement:**\n",
        "  * To focus on events characteristic of $t\\bar{t}$ decays, we require exactly one lepton (electron or muon) in the event.\n",
        "  * Purpose: This cut ensures that the event contains precisely one lepton, which is typical in semileptonic $t\\bar{t}$ decays where one top quark decays leptonically, and the other decays hadronically."
      ],
      "metadata": {
        "id": "eNis73fg37Xx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def one_lep(lep_n):\n",
        "    # Return True if exactly one lepton in the event\n",
        "    return lep_n == 1"
      ],
      "metadata": {
        "id": "-E7DleQT37y6"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **Lepton Transverse Momentum ($p_ùëá$) Cut:**\n",
        "  * High transverse momentum leptons are a key feature of $t\\bar{t}$ events.\n",
        "  * Purpose: This cut selects events where the transverse momentum of the lepton is above 30 GeV, ensuring that only significant lepton candidates are considered."
      ],
      "metadata": {
        "id": "E0vmYaGU38WB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cut_lep_pt(lep_pt):\n",
        "    # Return True if lepton pT is above 30 GeV and save a list of the lepton pT in GeV\n",
        "    Cut = lep_pt * MeV2GeV >= pt_lim\n",
        "    return Cut"
      ],
      "metadata": {
        "id": "C1P1qI2_4hB2"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. **Missing Transverse Energy ($E^{miss}_{T}$) Cut:**\n",
        "\n",
        "  * $t\\bar{t}$ events often have significant missing transverse energy due to the neutrinos produced in the decay.\n",
        "  * Purpose: This cut selects events where the missing transverse energy is greater than 30 GeV, helping to reduce background from processes without significant $E^{miss}_{T}$.\n"
      ],
      "metadata": {
        "id": "eWOAlMwZ4hfX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cut_met_et(met_et):\n",
        "    # Return True if missing ET is above 30 GeV and save a list of the missing ET in GeV\n",
        "    Cut = met_et * MeV2GeV >= 30\n",
        "    return Cut"
      ],
      "metadata": {
        "id": "8j0o3fdE4jNw"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. **W Boson Transverse Mass  ($M^{W}_{T}$) Cut:**\n",
        "  * The transverse mass of the W boson, reconstructed from the lepton transverse momentum and $E^{miss}_{T}$.\n",
        "  * Purpose: This cut ensures that the transverse mass of the W boson is above 30 GeV, further reducing background contamination."
      ],
      "metadata": {
        "id": "Pg7hrt2x4jvN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cut_W_mt(lep_pt, lep_phi, met_et, met_phi):\n",
        "    # Calculate W transverse mass and apply cut\n",
        "    Wmt = 2 * lep_pt * met_et * (1 - np.cos(lep_phi - met_phi)) * MeV2GeV**2\n",
        "    Wmt = ak.where(Wmt > 0, np.sqrt(Wmt), 0)\n",
        "    bool_list = Wmt >= 30\n",
        "    return bool_list"
      ],
      "metadata": {
        "id": "Ny7IW8CP4oJF"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. **Jet and b-Jet Multiplicity Cuts:**\n",
        "  * $t\\bar{t}$ events typically feature multiple jets, including those tagged as b-jets.\n",
        "  * Purpose: This cut requires at least four jets, with at least two of them identified as b-jets. This helps isolate the $t\\bar{t}$ signal from other processes that produce fewer jets or no b-jets."
      ],
      "metadata": {
        "id": "-D24AJBj4ouS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cut_Njet_and_Nbjet(jet_pt, jet_MV2c10):\n",
        "    # Convert jet pT to GeV\n",
        "    jet_pt = jet_pt * MeV2GeV\n",
        "\n",
        "    # Count jets above pT threshold\n",
        "    num_pt = ak.sum(jet_pt >= pt_lim, axis=1)\n",
        "\n",
        "    # Count b-tagged jets\n",
        "    num_btag = ak.sum(jet_MV2c10 >= MV2c10_lim, axis=1)\n",
        "\n",
        "    # Require at least 4 jets and 2 b-tagged jets\n",
        "    bool_list = (num_pt >= 4) & (num_btag >= 2)\n",
        "    return bool_list"
      ],
      "metadata": {
        "id": "kcsYwgHf4pGH"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Introduction to Mass Reconstruction in $t\\bar{t}$ Analysis**\n",
        "\n",
        "In the study of top-antitop quark pair production ($t\\bar{t}$), accurately reconstructing the masses of the decayed particles is essential for identifying the signal and distinguishing it from background processes. The top quark, the heaviest known elementary particle, typically decays into a W boson and a b-quark. In semileptonic $t\\bar{t}$ decays, one W boson decays into a lepton and a neutrino, while the other W boson decays hadronically into jets.\n",
        "\n",
        "To study these decays, we reconstruct the masses of the hadronically decaying top quark and the semileptonic top quark using the kinematic information of the detected particles. This process involves complex calculations that take into account the momentum, energy, and spatial distributions of the leptons, neutrinos, and jets involved in the decay.\n",
        "\n",
        "The following function, mtop, performs these calculations, reconstructing the mass of the top quarks in both their hadronic and semileptonic decay channels:\n",
        "\n",
        "**1. Leptonic Top Quark Mass ($M_{lvb}$)**\n",
        "* Purpose: The leptonic top quark mass is reconstructed using the lepton, the b-jet closest to the lepton, and the missing transverse energy (which is attributed to the neutrino).\n",
        "\n",
        "* Steps:\n",
        "  * Kinematic Conversions: Convert the energy and momentum of the lepton and neutrino from the detector units to GeV.\n",
        "\n",
        "  * Quadratic Solutions: Calculate the neutrino's longitudinal momentum ($p_{z}$) by solving a quadratic equation derived from the W boson mass constraint.\n",
        "\n",
        "  * Reconstruction: Combine the lepton, neutrino, and b-jet kinematics to compute the mass of the leptonic top quark for both solutions of $p_{z}$.\n",
        "2. Hadronic Top Quark Mass ($M_{jjj}$)\n",
        "\n",
        "* Purpose: The hadronic top quark mass is reconstructed using the b-jet farthest from the lepton and two other jets that form a pair with a mass close to that of the W boson.\n",
        "* Steps:\n",
        "  * Jet Pairing: Identify and pair jets that are not b-tagged, and calculate their invariant mass to find the pair closest to the W boson mass.\n",
        "  * Reconstruction: Combine the selected jet pair with the farthest b-jet to compute the mass of the hadronic top quark."
      ],
      "metadata": {
        "id": "HCJSrLwI81y_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mtop(jet_pt, jet_E, jet_eta, jet_phi, jet_MV2c10, lep_pt, lep_eta, lep_phi, lep_E, met_et, met_phi):\n",
        "    # This function calculates the mass of semileptonic decaying top quark and hadronic decay top quark\n",
        "\n",
        "    # Convert lepton kinematics\n",
        "    lep_E = lep_E * 1e-3\n",
        "    lep_pt = lep_pt * 1e-3\n",
        "    lep_px = lep_pt * np.cos(lep_phi)\n",
        "    lep_py = lep_pt * np.sin(lep_phi)\n",
        "    lep_pz = lep_pt / np.tan(2.0 * np.arctan( np.exp( -lep_eta ) ) )\n",
        "\n",
        "    # Convert neutrino kinematics\n",
        "    met_et = met_et * 1e-3\n",
        "    met_px = met_et * np.cos(met_phi)\n",
        "    met_py = met_et * np.sin(met_phi)\n",
        "\n",
        "    # Calculate neutrino pz solutions/\n",
        "    # Calculate coefficient 'a' of the quadratic equation\n",
        "    a = 4 * (lep_E**2 - lep_pz**2)\n",
        "\n",
        "    # Calculate coefficient 'b' of the quadratic equation, 6464.16 is (80.4 GeV)^2, which is the W boson mass squared\n",
        "    b = -4 * (6464.16 + ((lep_px + met_px)**2) +\n",
        "              ((lep_py + met_py)**2) - (lep_E**2) - (met_et**2) + (lep_pz**2)) * lep_pz\n",
        "\n",
        "    # Calculate coefficient 'c'\n",
        "    c = 4 * (lep_E**2) * (met_et**2) - (6464.16 + ((lep_px + met_px)**2) +\n",
        "               ((lep_py + met_py)**2) - (lep_E**2) - (met_et**2) + (lep_pz**2))**2\n",
        "\n",
        "    # Calculate the discriminant\n",
        "    Delta = b**2 - 4 * a * c\n",
        "\n",
        "    # Calculate the two solutions for met_pz using the quadratic formula\n",
        "    # If Delta is negative, set the solution to NaN\n",
        "    met_pz1 = ak.where(Delta >= 0, (-b + np.sqrt(Delta)) / ( 2 * a), np.NaN)\n",
        "    met_pz2 = ak.where(Delta >= 0, (-b - np.sqrt(Delta)) / ( 2 * a), np.NaN)\n",
        "\n",
        "    # Calculate the two solutions for met_E\n",
        "    met_E_1 = np.sqrt(met_px**2 + met_py**2 + met_pz1**2)\n",
        "    met_E_2 = np.sqrt(met_px**2 + met_py**2 + met_pz2**2)\n",
        "\n",
        "    # Convert jet kinematics to GeV\n",
        "    jet_E = jet_E * 1e-3\n",
        "    jet_pt = jet_pt * 1e-3\n",
        "    px = jet_pt * np.cos(jet_phi)\n",
        "    py = jet_pt * np.sin(jet_phi)\n",
        "    pz = jet_pt / np.tan(2.0 * np.arctan(np.exp(-jet_eta)))\n",
        "\n",
        "    # Identify b-tagged jets\n",
        "    b_tagged = jet_MV2c10 >= MV2c10_lim\n",
        "\n",
        "    # Separate b-tagged and non-b-tagged jets\n",
        "    b_tagged_px, b_tagged_py, b_tagged_pz, b_tagged_E = px[b_tagged], py[b_tagged], pz[b_tagged], jet_E[b_tagged]\n",
        "\n",
        "    b_tagged_eta, b_tagged_phi = jet_eta[b_tagged], jet_phi[b_tagged]\n",
        "\n",
        "    non_b_px, non_b_py, non_b_pz, non_b_E = px[~b_tagged], py[~b_tagged], pz[~b_tagged], jet_E[~b_tagged]\n",
        "\n",
        "\n",
        "    # Calculate delta R between lepton and b-tagged jets\n",
        "    lep_eta_broadcasted, _ = ak.broadcast_arrays(ak.flatten(lep_eta), b_tagged_eta)\n",
        "    lep_phi_broadcasted, _ = ak.broadcast_arrays(ak.flatten(lep_phi), b_tagged_phi)\n",
        "    temp_dR = (lep_eta_broadcasted - b_tagged_eta)**2 + (lep_phi_broadcasted - b_tagged_phi)**2\n",
        "    dR = ak.where(temp_dR >=0 ,np.sqrt(temp_dR),np.NaN)\n",
        "\n",
        "    # Find indices of closest and farthest b-tagged jets to the lepton\n",
        "    max_dR_indices = ak.singletons(ak.argmax(dR, axis=1))\n",
        "    min_dR_indices = ak.singletons(ak.argmin(dR, axis=1))\n",
        "\n",
        "    # Extract kinematics of closest b-tagged jets\n",
        "    closest_b_jet_E, closest_b_jet_px = b_tagged_E[min_dR_indices], b_tagged_px[min_dR_indices]\n",
        "    closest_b_jet_py, closest_b_jet_pz = b_tagged_py[min_dR_indices], b_tagged_pz[min_dR_indices]\n",
        "\n",
        "    # Extract kinematics of farthest b-tagged jets\n",
        "    farthest_b_jet_E, farthest_b_jet_px = b_tagged_E[max_dR_indices], b_tagged_px[max_dR_indices]\n",
        "    farthest_b_jet_py, farthest_b_jet_pz = b_tagged_py[max_dR_indices], b_tagged_pz[max_dR_indices]\n",
        "\n",
        "    # Helper function to create combinations of jets\n",
        "    def combo(list1):\n",
        "        jets_pairs = ak.combinations(list1, 2, fields=['List1', 'List2'])\n",
        "        sum_List = jets_pairs['List1'] + jets_pairs['List2']\n",
        "        return sum_List\n",
        "\n",
        "    # Create combinations of non-b-tagged jets\n",
        "    com_non_b_px, com_non_b_py = combo(non_b_px), combo(non_b_py)\n",
        "    com_non_b_pz, com_non_b_E = combo(non_b_pz), combo(non_b_E)\n",
        "\n",
        "    # Calculate mass difference from W boson mass\n",
        "    W_mass = np.sqrt(com_non_b_E**2 - (com_non_b_px**2 + com_non_b_py**2 + com_non_b_pz**2))\n",
        "    com_DM_W = 80.4 - W_mass\n",
        "    abs_diff = np.abs(com_DM_W)\n",
        "\n",
        "    # Select jet pairs close to W mass\n",
        "    check_abs = abs_diff <= 20\n",
        "    check_abs_array = ak.singletons(ak.any(check_abs, axis=1))\n",
        "\n",
        "    # Find best jet pair\n",
        "    min_diff_indices = ak.argmin(abs_diff, axis=1)\n",
        "    min_indices = ak.singletons(min_diff_indices)\n",
        "\n",
        "    # Extract kinematics of selected jet pair\n",
        "    sel_com_jet_E = com_non_b_E[min_indices]\n",
        "    sel_com_jet_px, sel_com_jet_py = com_non_b_px[min_indices], com_non_b_py[min_indices]\n",
        "    sel_com_jet_pz = com_non_b_pz[min_indices]\n",
        "\n",
        "    # Handle cases where no suitable jet pair is found\n",
        "    sel_com_jet_E = ak.without_parameters(ak.where(ak.num(sel_com_jet_E, axis=1) == 0, np.nan, sel_com_jet_E))\n",
        "    sel_com_jet_px = ak.without_parameters(ak.where(ak.num(sel_com_jet_px, axis=1) == 0, np.nan, sel_com_jet_px))\n",
        "    sel_com_jet_py = ak.without_parameters(ak.where(ak.num(sel_com_jet_py, axis=1) == 0, np.nan, sel_com_jet_py))\n",
        "    sel_com_jet_pz = ak.without_parameters(ak.where(ak.num(sel_com_jet_pz, axis=1) == 0, np.nan, sel_com_jet_pz))\n",
        "\n",
        "    # Calculate hadronic top\n",
        "    temp_m_jjj = ((sel_com_jet_E + farthest_b_jet_E)**2 - (sel_com_jet_px + farthest_b_jet_px)**2 -\n",
        "                  (sel_com_jet_py + farthest_b_jet_py)**2 - (sel_com_jet_pz + farthest_b_jet_pz)**2)\n",
        "\n",
        "    m_jjj = ak.where(temp_m_jjj>=0, np.sqrt(temp_m_jjj),np.nan)\n",
        "\n",
        "    # Apply W mass constraint\n",
        "    m_jjj = ak.where(check_abs_array, m_jjj, np.nan).tolist()\n",
        "\n",
        "    # Calculate leptonic top mass for both neutrino pz solutions\n",
        "    m_lvb_1 = ((lep_E + met_E_1 + closest_b_jet_E)**2 - ( (lep_px + met_px + closest_b_jet_px)**2 +\n",
        "     (lep_py + met_py + closest_b_jet_py)**2 + (lep_pz + met_pz1 + closest_b_jet_pz)**2 ))\n",
        "\n",
        "    m_lvb_2 = ((lep_E + met_E_2 + closest_b_jet_E)**2 - ( (lep_px + met_px + closest_b_jet_px)**2 +\n",
        "     (lep_py + met_py + closest_b_jet_py)**2 + (lep_pz + met_pz2 + closest_b_jet_pz)**2 ))\n",
        "\n",
        "    # Handle complex masses\n",
        "    m_lvb_1 = ak.where(m_lvb_1 >= 0, np.sqrt(m_lvb_2), np.nan).tolist()\n",
        "    m_lvb_2 = ak.where(m_lvb_2 >= 0, np.sqrt(m_lvb_2), np.nan).tolist()\n",
        "\n",
        "    # Flatten and return results\n",
        "    m_jjj, m_lvb_1, m_lvb_2 = ak.flatten(m_jjj), ak.flatten(m_lvb_1), ak.flatten(m_lvb_2)\n",
        "    return [m_jjj, m_lvb_1, m_lvb_2]"
      ],
      "metadata": {
        "id": "faNP7dAe39Es"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Processing and Event Selection Function**\n",
        "\n",
        "The read_file function is designed to read and process data from a file, apply selection cuts, and return an awkward array containing the events that pass all cuts."
      ],
      "metadata": {
        "id": "xkXAGR_I7jVq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_file(path,sample,loop):\n",
        "    \"\"\"\n",
        "    Reads data from a file, applies cuts, and returns an awkward array.\n",
        "\n",
        "    Args:\n",
        "        path (str): Path to the file.\n",
        "        sample (str): Sample name.\n",
        "        loop (int): Starting entry for data retrieval.\n",
        "\n",
        "    Returns:\n",
        "        ak.Array: Concatenated awkward array containing events passing all cuts.\n",
        "    \"\"\"\n",
        "    data_all = [] # define empty list to hold all data for this sample\n",
        "\n",
        "    # Open the tree called \"mini\" using a context manager (will automatically close files/resources)\n",
        "    with uproot.open(f\"{path}:mini\") as tree:\n",
        "        numevents = tree.num_entries  # Number of events\n",
        "        for data in tree.iterate(\n",
        "                  [\n",
        "                  \"mcWeight\", \"scaleFactor_ELE\", \"scaleFactor_MUON\", \"scaleFactor_LepTRIGGER\",\n",
        "                  \"scaleFactor_PILEUP\", \"scaleFactor_BTAG\", \"trigE\", \"trigM\", \"lep_n\", \"lep_pt\",\n",
        "                  \"lep_eta\", \"lep_phi\", \"lep_charge\", \"lep_type\", \"met_et\", \"met_phi\", \"SumWeights\",\n",
        "                  \"XSection\", \"jet_pt\", \"jet_MV2c10\", \"jet_n\", \"jet_eta\", \"jet_phi\", \"jet_E\",\n",
        "                  \"lep_type\", \"lep_E\", \"eventNumber\"\n",
        "                  ],\n",
        "                  library=\"ak\",  # Choose output type as awkward array\n",
        "                  entry_start = int( round(numevents * fraction * loop ,0) ),\n",
        "                  entry_stop = int( round(numevents * fraction *(loop+1),0) )\n",
        "                  ):\n",
        "\n",
        "                  nIn = len(data) # number of events in this batch\n",
        "\n",
        "                  # Check triggerE, triggerM  conditions\n",
        "                  data = data[cut_trig(data.trigE,data.trigM)]\n",
        "\n",
        "                  # Require exactly one lepton\n",
        "                  data = data[one_lep(data.lep_n)]\n",
        "\n",
        "                  # Cut on lepton lep_pt\n",
        "                  data = data[ak.flatten(cut_lep_pt(data.lep_pt))]\n",
        "\n",
        "                  # Cut on missing transverse energy met_et\n",
        "                  data = data[cut_met_et(data.met_et)]\n",
        "\n",
        "                  data = data[ak.flatten(cut_W_mt(data.lep_pt, data.lep_phi, data.met_et, data.met_phi))]\n",
        "\n",
        "                  data = data[cut_Njet_and_Nbjet(data.jet_pt,data.jet_MV2c10)]\n",
        "\n",
        "                  if 'data' not in sample: # only do this for Monte Carlo simulation files\n",
        "                      # multiply all Monte Carlo weights and scale factors together to give total weight\n",
        "                      data['weight'] = calc_weight(data)\n",
        "\n",
        "                  else: data['weight'] =  ak.zeros_like(data['met_et'])\n",
        "\n",
        "                  mtop_data = mtop(data.jet_pt ,data.jet_E ,data.jet_eta ,data.jet_phi ,data.jet_MV2c10 ,\n",
        "                                  data.lep_pt, data.lep_eta ,data.lep_phi, data.lep_E, data.met_et, data.met_phi)\n",
        "\n",
        "                  data['mtop'] = mtop_data[0]\n",
        "\n",
        "                  data['mtop_1'] = mtop_data[1]\n",
        "\n",
        "                  data['mtop_2'] = mtop_data[2]\n",
        "\n",
        "                  nOut = len(data) # number of events passing cuts in this batch\n",
        "                  data_all.append(data) # append array from this batch\n",
        "                  del(data)\n",
        "\n",
        "    return ak.concatenate(data_all) # return array containing events passing all cuts"
      ],
      "metadata": {
        "id": "ie6UR2Vc7kGW"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Retrieval in $t\\bar{t}$ Analysis**\n",
        "\n",
        "This function plays a vital role in handling the different datasets required for a comprehensive $t\\bar{t}$ analysis, including both Monte Carlo (MC) simulation samples and actual experimental data. By organizing the data into a dictionary of awkward arrays, it ensures that the subsequent analysis steps can be performed smoothly and efficiently."
      ],
      "metadata": {
        "id": "xl6qQ7k57kaS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data_from_files(type_data, loop):\n",
        "    \"\"\"\n",
        "    Retrieves data from files and returns a dictionary of awkward arrays.\n",
        "\n",
        "    Args:\n",
        "        type_data (str): A string specifying the type of data to retrieve.\n",
        "        loop (int): The starting point for data retrieval.\n",
        "\n",
        "    Returns:  dict: A dictionary containing concatenated awkward arrays for different samples.\n",
        "    \"\"\"\n",
        "    data = {}\n",
        "    frames = []  # Define an empty list to hold data\n",
        "    for val in samples[type_data][\"list\"]:  # Loop over each file\n",
        "        tuple_path = \"https://atlas-opendata.web.cern.ch/atlas-opendata/samples/2020/1lep/\"\n",
        "        if val == \"data_A\" or val == \"data_B\" or val == \"data_C\" or val == \"data_D\":\n",
        "            prefix = \"Data/\"\n",
        "        elif val != \"data\":\n",
        "            prefix = \"MC/\"\n",
        "        #print(val)\n",
        "        fileString = f\"{tuple_path}{prefix}{val}.1lep.root\"  # File name to open\n",
        "        frames.append(read_file(fileString, val, loop))  # Append array returned from read_file\n",
        "    frames = ak.flatten(frames)\n",
        "    return frames  # Return the dictionary of awkward arrays"
      ],
      "metadata": {
        "id": "0olSZnYW7k9o"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Processing**\n",
        "This function is designed to retrieve and process data for a given type (e.g., \"MC\" or \"data\") and organize it into a Pandas DataFrame. This function performs batch processing, where data is retrieved from files iteratively, allowing for efficient handling of large datasets. The DataFrame includes key variables such as event weights and the top quark masses from leptonic decays and hadronic decays."
      ],
      "metadata": {
        "id": "WEkRLiTyYQqd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analysis(data_type,loop):\n",
        "    \"\"\"\n",
        "    Processes data for a given data type and loop, extracting specific columns and converting\n",
        "    them into a Pandas DataFrame.\n",
        "\n",
        "    Args:\n",
        "        data_type (str): The type of data to process (e.g., \"MC\", \"data\").\n",
        "        loop (int): The starting point for data retrieval (used for batch processing).\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A Pandas DataFrame containing extracted data columns ('Weight', 'mtop', 'mtop2', 'mtop3').\n",
        "    \"\"\"\n",
        "    # Process data for data_type sample\n",
        "    data = get_data_from_files(data_type,loop)\n",
        "    data_df = pd.DataFrame({\n",
        "            \"Weight\": ak.to_list(data[data_type]['weight']),\n",
        "            \"mtop\": ak.to_list(data[data_type]['mtop']),\n",
        "            \"mtop2\": ak.to_list(data[data_type]['mtop_1']),\n",
        "            \"mtop3\": ak.to_list(data[data_type]['mtop_2'])\n",
        "        })\n",
        "    del(data)       # Delete the 'data' dictionary to free up memory\n",
        "    return data_df  # Return the created Pandas DataFrame"
      ],
      "metadata": {
        "id": "RzAHJWpoYQ9j"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Aggregation with Loops**\n",
        "The code below runs over multiple datasets and concatenates the results into Pandas DataFrames. It performs this for a specified number of iterations (loops) to aggregate data from multiple files.\n",
        "\n",
        "This loop-based approach allows the processing of data in batches, making it scalable and memory-efficient for handling large datasets."
      ],
      "metadata": {
        "id": "BxHUc6f7mA3r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loops = 9             # reduce this if you want the code to run quicker (1-9)\n",
        "start = time.time()  # Time at the start of the whole processing\n",
        "\n",
        "main_analysis_df = analysis(\"data_A\",0)\n",
        "main_analysis_df = pd.concat([analysis(\"data_B\",0), main_analysis_df],ignore_index=True)\n",
        "main_analysis_df = pd.concat([analysis(\"data_C\",0), main_analysis_df],ignore_index=True)\n",
        "main_analysis_df = pd.concat([analysis(\"data_D\",0), main_analysis_df],ignore_index=True)\n",
        "\n",
        "ttbar_data_df = analysis(\"ttbar\",0)\n",
        "single_top_data_df = analysis(\"Single top\",0)\n",
        "dilepton_data_df = analysis(\"Diboson\",0)\n",
        "\n",
        "elapsed = time.time() - start  # time after whole processing\n",
        "print(f\"Time taken: {round(elapsed/60,1)} min, {round(fraction*100,1)}%\")  # print total time taken to process every file\n",
        "\n",
        "if loops > 1:\n",
        "    for i in range(loops):\n",
        "      start = time.time()  # Time at the start of the whole processing\n",
        "\n",
        "      main_analysis_df = pd.concat([analysis(\"data_A\",i+1), main_analysis_df],ignore_index=True)\n",
        "      main_analysis_df = pd.concat([analysis(\"data_B\",i+1), main_analysis_df],ignore_index=True)\n",
        "      main_analysis_df = pd.concat([analysis(\"data_C\",i+1), main_analysis_df],ignore_index=True)\n",
        "      main_analysis_df = pd.concat([analysis(\"data_D\",i+1), main_analysis_df],ignore_index=True)\n",
        "\n",
        "      ttbar_data_df = pd.concat([analysis(\"ttbar\",i+1), ttbar_data_df],ignore_index=True)\n",
        "      single_top_data_df = pd.concat([analysis(\"Single top\",i+1), single_top_data_df],ignore_index=True)\n",
        "      dilepton_data_df = pd.concat([analysis(\"Diboson\",i+1), dilepton_data_df],ignore_index=True)\n",
        "\n",
        "      elapsed = time.time() - start  # time after whole processing\n",
        "      print(f\"Time taken: {round(elapsed/60,1)} min, {round(fraction*(i+2)*100,1)}%\")  # print total time taken to process every file"
      ],
      "metadata": {
        "id": "FCxJ_a9_fDN3",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Combining Mass and Weight Lists**\n",
        "In the context of $t\\bar{t}$ analysis, when reconstructing the mass of the top quark from the leptonic decay, we often encounter two possible solutions for the neutrino's momentum due to the nature of the quadratic equation involved in the reconstruction. These two solutions lead to two potential values for the top quark mass. To account for both possibilities in the analysis, we combine these two mass values into a single list. However, since both solutions are considered equally probable, the weight for each event must be divided by 2 to avoid double counting\n"
      ],
      "metadata": {
        "id": "OnCW2QpOlThc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine mtop2 and mtop3 lists main_analysis_df\n",
        "mtop2 = ak.flatten([main_analysis_df[\"mtop2\"], main_analysis_df[\"mtop3\"]])\n",
        "weights_data = ak.Array([1/2] * 2 * len(main_analysis_df))\n",
        "\n",
        "# Combine MC-related lists\n",
        "mtop2_ttbar = ak.flatten([ttbar_data_df[\"mtop2\"], ttbar_data_df[\"mtop3\"]])\n",
        "weights_ttbar = ak.flatten([ttbar_data_df[\"Weight\"], ttbar_data_df[\"Weight\"]]) / 2\n",
        "\n",
        "# Combine Single top-related lists\n",
        "mtop2_ST = ak.flatten([single_top_data_df[\"mtop2\"], single_top_data_df[\"mtop3\"]])\n",
        "weights_ST = ak.flatten([single_top_data_df[\"Weight\"], single_top_data_df[\"Weight\"]]) / 2\n",
        "\n",
        "# Combine Diboson-related lists\n",
        "mtop2_Di = ak.flatten([dilepton_data_df[\"mtop2\"], dilepton_data_df[\"mtop3\"]])\n",
        "weights_Di = ak.flatten([dilepton_data_df[\"Weight\"], dilepton_data_df[\"Weight\"]]) / 2"
      ],
      "metadata": {
        "id": "TyGnrgbdkvDX"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Plotting**\n",
        "\n",
        "Define function to plot the data"
      ],
      "metadata": {
        "id": "2cuk0LiPlTQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_data(data, data_MC, MC_Weight, data_Di, Di_Weight, data_ST, ST_Weight,\n",
        "              x_axis_label, weight_for_data, weight_data):\n",
        "    \"\"\"\n",
        "    Plot histogram data with Monte Carlo simulations and calculate ratio plot.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    data : array\n",
        "        The main data to be plotted.\n",
        "    data_MC, data_Di, data_ST : array\n",
        "        Monte Carlo, Diboson, and Single Top data respectively.\n",
        "    MC_Weight, Di_Weight, ST_Weight : array-like\n",
        "        Weights for the respective data sets.\n",
        "    x_axis_label : str\n",
        "        Label for the x-axis.\n",
        "    weight_for_data : bool\n",
        "        Flags for additional functionality.\n",
        "    weight_data : array\n",
        "        Weights for the main data.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    None\n",
        "    \"\"\"\n",
        "\n",
        "    # Define plot parameters\n",
        "    xmin, xmax, step_size = 100, 250, 3\n",
        "\n",
        "    # Define MC data sets and their properties\n",
        "    datasets = [\n",
        "        {'data': data_Di, 'weights': Di_Weight, 'color': 'blue', 'label': r'Diboson'},\n",
        "        {'data': data_ST, 'weights': ST_Weight, 'color': 'cyan', 'label': r'Single top'},\n",
        "        {'data': data_MC, 'weights': MC_Weight, 'color': 'orange', 'label': r'$t\\bar{t}$'}\n",
        "    ]\n",
        "\n",
        "    # Create bin edges and centers\n",
        "    bin_edges = np.arange(xmin, xmax + step_size, step_size)\n",
        "    bin_centres = np.arange(xmin + step_size/2, xmax + step_size/2, step_size)\n",
        "\n",
        "    # Histogram the data and weights for the data\n",
        "    if weight_for_data:\n",
        "        data_x, _ = np.histogram(ak.to_numpy(data), bins=bin_edges, weights=weight_data)\n",
        "    else:\n",
        "        data_x, _ = np.histogram(ak.to_numpy(data), bins=bin_edges)\n",
        "\n",
        "    data_x_errors = np.sqrt(data_x)  # statistical error on the data\n",
        "\n",
        "    # Create main plot and residual subplot\n",
        "    fig, (main_axes, residual_axes) = plt.subplots(2, 1, figsize=(7, 6), gridspec_kw={'height_ratios': [3, 1]}, sharex=True)\n",
        "\n",
        "    # Plot data with error bars\n",
        "    main_axes.errorbar(x=bin_centres, y=data_x, yerr=data_x_errors,\n",
        "                       fmt='ko', label='Data')\n",
        "\n",
        "    # Plot the Monte Carlo bars\n",
        "    mc_heights = main_axes.hist([d['data'] for d in datasets], bins=bin_edges,\n",
        "                                weights=[d['weights'] for d in datasets], stacked=True,\n",
        "                                color=[d['color'] for d in datasets], label=[d['label'] for d in datasets])\n",
        "\n",
        "    mc_x_tot = mc_heights[0]  # Stacked background MC y-axis value\n",
        "\n",
        "    # Calculate MC statistical uncertainty: sqrt(sum w^2)\n",
        "    mc_x_err = np.sqrt(np.histogram(np.hstack([d['data'] for d in datasets]), bins=bin_edges,\n",
        "                                    weights=np.hstack([d['weights'] for d in datasets])**2)[0])\n",
        "\n",
        "    # Plot the statistical uncertainty\n",
        "    main_axes.bar(bin_centres, 2*mc_x_err, alpha=0.5, bottom=mc_x_tot[2]-mc_x_err,\n",
        "                  color='none', hatch=\"////\", width=step_size, label='Stat. Unc.')\n",
        "\n",
        "    # Set up main axes\n",
        "    main_axes.set_xlim(left=xmin, right=xmax)\n",
        "\n",
        "    # Add headspace to the plot\n",
        "    ymax = max(np.max(data_x), np.max(np.sum(mc_heights[0], axis=0)))\n",
        "    main_axes.set_ylim(0, ymax * 1.4)  # Add 40% headspace\n",
        "\n",
        "    main_axes.xaxis.set_minor_locator(AutoMinorLocator())\n",
        "    main_axes.tick_params(which='both', direction='in', top=True, right=True)\n",
        "    main_axes.set_ylabel('Events', y=1, horizontalalignment='right')\n",
        "    main_axes.yaxis.set_minor_locator(AutoMinorLocator())\n",
        "\n",
        "    # Add text to the plot\n",
        "    main_axes.text(0.05, 0.93, 'ATLAS Open Data', transform=main_axes.transAxes, fontsize=13)\n",
        "    main_axes.text(0.05, 0.88, 'for education', transform=main_axes.transAxes, style='italic', fontsize=8)\n",
        "    main_axes.text(0.05, 0.82, r'$\\sqrt{s}$=13 TeV, 10 fb$^{-1}$', transform=main_axes.transAxes)\n",
        "    main_axes.text(0.05, 0.76, r'$t\\bar{t} \\rightarrow \\ell v b\\bar{b} q\\bar{q}$', transform=main_axes.transAxes)\n",
        "\n",
        "    main_axes.legend(frameon=False)\n",
        "\n",
        "    # Calculate and plot residuals\n",
        "    ratio = data_x / np.sum(mc_heights[0], axis=0)\n",
        "    residual_axes.errorbar(bin_centres, ratio, fmt='ko')\n",
        "    residual_axes.axhline(1, color='r', linestyle='--')\n",
        "    residual_axes.set_ylim(0.5,1.5)\n",
        "    residual_axes.set_xlabel(x_axis_label, fontsize=13, x=1, horizontalalignment='right')\n",
        "    residual_axes.set_ylabel('Ratio (Data/MC)')\n",
        "    residual_axes.xaxis.set_minor_locator(AutoMinorLocator())\n",
        "    residual_axes.yaxis.set_minor_locator(AutoMinorLocator())\n",
        "    residual_axes.tick_params(which='both', direction='in', top=True, right=True)\n",
        "\n",
        "    # Adjust layout\n",
        "    fig.tight_layout()\n",
        "    fig.subplots_adjust(hspace=0.05)\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "R81AK9L_fDK_"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Call the function to plot the data"
      ],
      "metadata": {
        "id": "gFvcCpnjlYd9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the trijet mass distribution (m_jjj)\n",
        "plot_data(ak.Array(main_analysis_df['mtop']),       # Main data: list of trijet masses\n",
        "          ak.Array(ttbar_data_df['mtop']),          # Monte Carlo data for ttbar\n",
        "          ak.Array(ttbar_data_df['Weight']),        # Weights for Monte Carlo data\n",
        "          ak.Array(dilepton_data_df['mtop']),       # Diboson background data\n",
        "          ak.Array(dilepton_data_df['Weight']),     # Weights for Diboson data\n",
        "          ak.Array(single_top_data_df['mtop']),     # Single Top background data\n",
        "          ak.Array(single_top_data_df['Weight']),   # Weights for Single Top data\n",
        "          r\"$\\mathrm{m_{jjj}} [GeV]$\",              # x-axis label: trijet mass in GeV\n",
        "          False,                                    # Don't use weights for the main data\n",
        "          [])                                       # Empty list for data weights (not used)\n",
        "\n",
        "# Plot the lepton-neutrino-b-jet mass distribution (m_lvb)\n",
        "plot_data(mtop2,                        # Main data: list of lepton-neutrino-b-jet masses\n",
        "          mtop2_ttbar,                  # Monte Carlo data for ttbar\n",
        "          weights_ttbar,                # Weights for Monte Carlo data\n",
        "          mtop2_Di,                     # Diboson background data\n",
        "          weights_Di,                   # Weights for Diboson data\n",
        "          mtop2_ST,                     # Single Top background data\n",
        "          weights_ST,                   # Weights for Single Top data\n",
        "          r\"$\\mathrm{m_{lvb}} [GeV]$\",  # x-axis label: lepton-neutrino-b-jet mass in GeV\n",
        "          True,                         # Use weights for the main data\n",
        "          weights_data)                 # Weights for the main data"
      ],
      "metadata": {
        "id": "HA0ew410fDHr",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
